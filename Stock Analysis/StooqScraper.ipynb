{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e5be0de",
   "metadata": {},
   "source": [
    "### Stocks Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebdb415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from datetime import date\n",
    "import time\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2967d393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function generates hyperlinks to stooq.pl website. Ticker, first and last observation date and number of pages need to be provided\n",
    "#If number of pages won't be provided 20 hyperlinks will be created by default\n",
    "#(Take into consideration that if more historical data need to be downloaded default no of pages might not be sufficient)\n",
    "def generateHyperlinks(ticker,startDate = \"2020-01-01\",endDate = str(date.today()),no_of_pages = 20):\n",
    "    startDate = startDate.replace(\"-\",\"\")\n",
    "    endDate = endDate.replace(\"-\",\"\")\n",
    "    webpage = r\"https://stooq.pl/q/d/?s=\" + ticker + '&c=0&d1=' + startDate + \"&d2=\" + endDate + \"&l=\"\n",
    "    hyperlinks = [webpage + str(i) for i in range(1,no_of_pages+1)]\n",
    "    return hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7dfcad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion which iterates through websites and save share prices from each page to seperate list argument\n",
    "#Input: list of hyperlinks to iterate\n",
    "#Output: list with unordered data collected from each 'subpage'\n",
    "def pageIterator(hyperlinks):\n",
    "    driver = webdriver.Chrome(executable_path=r\"C:\\Users\\woote\\Desktop\\Python\\Projects\\chromedriver.exe\")\n",
    "    tableValues = []\n",
    "    #Remember that Stooq.pl has a limit of daily page refreshes\n",
    "    for i,link in enumerate(hyperlinks):\n",
    "\n",
    "        driver.get(link)\n",
    "\n",
    "        if i == 0:\n",
    "            try:\n",
    "                accept = driver.find_element_by_xpath(\"//p[contains(@class, 'fc-button-label')]\")\n",
    "                accept.click()\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(3)\n",
    "\n",
    "        try:\n",
    "            tableText = driver.find_element_by_id(\"fth1\")\n",
    "            tableValues.append(tableText.text)\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    return tableValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc20b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function which converts a list of unordered list of tables extracted from HTML code\n",
    "#Input: a list of values\n",
    "#Output: Clean pandas data frame\n",
    "def dataCleaner(tableValues, colnames = ['id','day','month','year','open','max','min','close','perc_change','abs_change','volume']):\n",
    "    \n",
    "    #Dictionary which will help to convert month names to a digits in date column\n",
    "    months = {\"sty\": \"01\",\"lut\": \"02\",\"mar\": \"03\",\"kwi\": \"04\",\"maj\": \"05\",\"cze\": \"06\",\\\n",
    "              \"lip\": \"07\",\"sie\": \"08\",\"wrz\": \"09\",\"pa≈∫\": \"10\",\"lis\": \"11\",\"gru\": \"12\"}\n",
    "    #Joining list elements with new line operator, then deliminating by spaces\n",
    "    #Dropping the duplicated rows (as we are scraping column headers as well)\n",
    "    #And erasing first row wiith wrongly assigned headers\n",
    "    #Deleting rows where 'close' is nan (usually its additional row pointing the date of dividend)\n",
    "    data = pd.read_csv(io.StringIO('\\n'.join(tableValues)), delim_whitespace=True,names=colnames).drop_duplicates().iloc[1:,:]\n",
    "    data = data[data['close'].notna()]\n",
    "    data = data.iloc[:-1 , :]\n",
    "    #Reseting the index (after deleting duplicated rows)\n",
    "    data.reset_index(inplace = True)\n",
    "    #Inserting date column\n",
    "    data.insert(4,'date',pd.to_datetime(data[\"day\"] + \"/\" + data[\"month\"].map(months) + \"/\" + data[\"year\"], format='%d/%m/%Y'))\n",
    "    #Dropping needless columns\n",
    "    data.drop(['index','id','day','month','year'],axis = 1,inplace = True)\n",
    "    #Adjusting data types\n",
    "    data[['max','min','open','close']] = data[['max','min','open','close']].astype(float)\n",
    "    data['volume'] = data['volume'].str.replace(\",\",\"\").fillna(0).astype(int)\n",
    "    data['perc_change'] = data['perc_change'].str.replace(\"%\",\"\").str.replace(\"+\",\"\").astype(float)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00350f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-5688c3e7e01f>:25: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  data['perc_change'] = data['perc_change'].str.replace(\"%\",\"\").str.replace(\"+\",\"\").astype(float)\n"
     ]
    }
   ],
   "source": [
    "hyperlinks = generateHyperlinks(\"MVP\",\"2021-01-01\",no_of_pages=8)\n",
    "tableValues = pageIterator(hyperlinks)\n",
    "df = dataCleaner(tableValues)\n",
    "df.to_excel('Share_prices_Marvipol.xlsx',sheet_name='Data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
